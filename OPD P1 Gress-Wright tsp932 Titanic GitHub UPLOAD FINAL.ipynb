{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPEN DATA SCIENCE\n",
    "## PORTFOLIO 1 Titanic\n",
    "Gress-Wright tsp932\n",
    "5 January 2020\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTRODUCTION\n",
    "\n",
    "This Portfolio I consists of this introduction, an analysis, Python computer code and its output.\n",
    "\n",
    "The task of Portfolio I involves analyzing data on passengers on the Titanic. There are several steps involved, including transforming, checking and teasing out the necessary data, describing the resulting data file, followed by the creation of descriptive statistics and then data analysis.  The Python computer code was originally produced in the development environment Spyder. The final version here is in Jupyter Notebook.\n",
    "\n",
    "In the course overall, we have learned to work with large quantities of data using Python, a high level programming language. We have used it to generate descriptive statistics and for text mining.  Python and its methods are a toolkit, a means to a research end (Downey, 2015; Ignatow and Milhacea, 2018). They help the practice of Open Data Science. This Introduction therefore sets the portfolio as a whole in the context of Open Data Science (Wilkinson, et al., 2016).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ABOUT OPEN DATA SCIENCE: BACKGROUND, KNOWLEDGE ORGANIZATION AND ETHICS\n",
    "\n",
    "Open Science is defined as “transparent and accessible knowledge that is shared and developed through collaborative networks” (Vicente-Saez & Martinez-Fuentes 2018, abstract).\n",
    "\n",
    "Open Data Science is the application of Open Data Science (ODS) principles to data science and the entire research life cycle (from idea gathering to final publication) from day one. Thus ODS facilitates both on-line research collaboration using open-source software and large, digitally accessible data sets but also replication and dissemination of results without unnecessary duplication (Lund, 2019).  \n",
    "\n",
    "Technological and institutional forces both drive the transition to Open Data Science but also hinder it. Data are increasingly to be found, and required to be available, in \"computer artifacts” e.g. online lab notebooks, code, scripts, datasets, algorithms and software (Association for Computing Machinery, 2018). Funders increasingly demand, not a final product such as a project report or a published article, but access to the underlying data itself. On the other hand, the publish or perish culture of universities favors keeping one's data to oneself.  Technology makes security and privacy difficult to ensure and commercial pressures may favor unethical practices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FAIR  is the attempt to “improve the infrastructure supporting the reuse of scholarly data” (Wilkinson et al., 2016: 1). FAIR is designed with digital technology in mind.   “Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals.” (Wilkinson et al., 2016: 1).  FAIR is applied to the research cycle and helps to determine what practices and tools to use at each stage (Lund, 2019). Infrastructure should ensure that data is:\n",
    "\n",
    "    Findable\n",
    "    Accessible\n",
    "    Interoperable (important!)\n",
    "    Refindable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETHICS IN OPEN DATA SCIENCE\n",
    "\n",
    "As well, there are ethical considerations in Open Science and Open Data Science. These are expressed in the ACM Code of Ethics and Professional Conduct.  The Code as a whole is \"concerned with how fundamental ethical principles apply to a computing professional's conduct... [and]...serves as a basis for ethical decision-making.” (Anderson, 1992).  The six core principles are:\n",
    "\n",
    "1.1 Contribute to society and to human well-being, acknowledging that all people are stakeholders in computing.\n",
    "1.2 Avoid harm.\n",
    "1.3 Be honest and trustworthy.\n",
    "1.4 Be fair and take action not to discriminate.\n",
    "1.5 Respect the work required to produce new ideas, inventions, creative works, and computing artifacts.\n",
    "1.6 Respect privacy.\n",
    "\n",
    "In practice, these include, for example, the duty to secure informed consent and to ensure privacy (Lund, 2019). \n",
    "\n",
    "Yet, in a digital world,  following these principles is not always easy when are commercial, political or even criminal pressures involved.  For example,  Principle 1.6  requires that sensitive personal information not be traceable to the individuals involved.  Data should be anonymized.  Yet a New York Times investigation of a single data leak (50 billion pings of app-derived location data) revealed that the data, contrary to formal policy, was neither anonymous nor secure (Thompson and Warzel, 2019). These pressures mean that a personal commitment to ethical conduct is more important than ever.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASSIGNMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I open the data file \"titanic.csv\" in a texteditor using Windows Notepad and look at the contents. This allows me to visually inspect all the data and datatypes. There are integers, floats and strings.\n",
    "The actual data seem complete for the 887 persons. BUT there were 2224 passengers and crew so data for about three fifths of the passengers is missing.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I import pandas. Next, I create the variable \"df\" for the dataframe. I do not specify a separator because this is a .csv file and pandas assume comma separators by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1:  Import pandas\n",
      "     Survived  Pclass                                               Name  \\\n",
      "0           0       3                             Mr. Owen Harris Braund   \n",
      "1           1       1  Mrs. John Bradley (Florence Briggs Thayer) Cum...   \n",
      "2           1       3                              Miss. Laina Heikkinen   \n",
      "3           1       1        Mrs. Jacques Heath (Lily May Peel) Futrelle   \n",
      "4           0       3                            Mr. William Henry Allen   \n",
      "..        ...     ...                                                ...   \n",
      "882         0       2                               Rev. Juozas Montvila   \n",
      "883         1       1                        Miss. Margaret Edith Graham   \n",
      "884         0       3                     Miss. Catherine Helen Johnston   \n",
      "885         1       1                               Mr. Karl Howell Behr   \n",
      "886         0       3                                 Mr. Patrick Dooley   \n",
      "\n",
      "        Sex   Age  Siblings/Spouses Aboard  Parents/Children Aboard     Fare  \n",
      "0      male  22.0                        1                        0   7.2500  \n",
      "1    female  38.0                        1                        0  71.2833  \n",
      "2    female  26.0                        0                        0   7.9250  \n",
      "3    female  35.0                        1                        0  53.1000  \n",
      "4      male  35.0                        0                        0   8.0500  \n",
      "..      ...   ...                      ...                      ...      ...  \n",
      "882    male  27.0                        0                        0  13.0000  \n",
      "883  female  19.0                        0                        0  30.0000  \n",
      "884  female   7.0                        1                        2  23.4500  \n",
      "885    male  26.0                        0                        0  30.0000  \n",
      "886    male  32.0                        0                        0   7.7500  \n",
      "\n",
      "[887 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "print('STEP 1:  Import pandas') \n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"titanic.csv\") \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2a. Data types\n",
    "\n",
    "i. Datatype of data frame itself \n",
    "To find the variable type for df itself, I use the 'type' function. 'df' is a <class 'pandas.core.frame.DataFrame'>\n",
    "\n",
    "ii. Data types of columns \n",
    "Next, I used print(df.dtypes) and (df.info()), which gives data type by column, and sums up the total of each data type. It showed integers, floats, and strings (e.g. Survived cell datatype was 0 or 1 i.e. integer, while Name cell data type was a string - identified as an 'object')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Survived                     int64\n",
      "Pclass                       int64\n",
      "Name                        object\n",
      "Sex                         object\n",
      "Age                        float64\n",
      "Siblings/Spouses Aboard      int64\n",
      "Parents/Children Aboard      int64\n",
      "Fare                       float64\n",
      "dtype: object\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 887 entries, 0 to 886\n",
      "Data columns (total 8 columns):\n",
      "Survived                   887 non-null int64\n",
      "Pclass                     887 non-null int64\n",
      "Name                       887 non-null object\n",
      "Sex                        887 non-null object\n",
      "Age                        887 non-null float64\n",
      "Siblings/Spouses Aboard    887 non-null int64\n",
      "Parents/Children Aboard    887 non-null int64\n",
      "Fare                       887 non-null float64\n",
      "dtypes: float64(2), int64(4), object(2)\n",
      "memory usage: 55.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('Data types')\n",
    "print(type(df))\n",
    "print(df.dtypes)\n",
    "print(df.info()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2b. Shape of the dataframe(numbers of rows and colums) First I ask how many rows, then how many rows and columns (returned as a tuple) and how many cells. I then ask what are the columns' names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe the dataframe\n",
      "Number of rows:  887\n",
      "Number of rows and columns:  (887, 8)\n",
      "Number of cells:  7096\n",
      "Column names:  Index(['Survived', 'Pclass', 'Name', 'Sex', 'Age', 'Siblings/Spouses Aboard',\n",
      "       'Parents/Children Aboard', 'Fare'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print('Describe the dataframe') \n",
    "print('Number of rows: ', len(df))\n",
    "print('Number of rows and columns: ', df.shape)\n",
    "print('Number of cells: ', df.size)\n",
    "print('Column names: ', df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3)  \n",
    "\n",
    "3.1 I first extract chosen data into a new file. I then calculate, using thosedata, the descriptive statistics for the 887 passengers. I chose to do this by creating a pivot table from dataframe df.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3a. The number of surviving passengers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe the passengers\n",
      "The number of surviving passengers\n",
      "Surviving passengers (0 = dead, 1 = survived)\n",
      "          Name\n",
      "         count\n",
      "Survived      \n",
      "0          545\n",
      "1          342\n",
      "_____________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Describe the passengers')\n",
    "print('The number of surviving passengers')\n",
    "\n",
    "pivot1=pd.pivot_table(df, index=['Survived'], values=['Name'], aggfunc={'count'})\n",
    "print('Surviving passengers (0 = dead, 1 = survived)')\n",
    "print(pivot1)\n",
    "print('_____________________________________________________________________') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pivot_table function indexed (i.e. sorted) the lines (i.e. records) by the values in column \"Survived\" (i.e. 0 died, 1 survived). This gave two groups. Values is taken from column \"Name\", in order to count something which was not a number. Aggfunc={count} sums up the total number of names in each group 0 = 545, 1 = 342. The number of surviving passengers is 342.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3b. The mean age for ALL passengers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean age, for ALL passengers\n",
      "29.471443066516347\n",
      "_____________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mean = df.mean(axis=0)\n",
    "print('Mean age, for ALL passengers')\n",
    "print(mean['Age'])\n",
    "print('_____________________________________________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3c. The median age for ALL passengers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median age, for ALL passengers\n",
      "28.0\n",
      "_____________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "median = df.median(axis=0)\n",
    "print('Median age, for ALL passengers')\n",
    "print(median['Age'])\n",
    "print('_____________________________________________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3d.  EXTRA The mean and median age for passengers, by survival status\n",
    "Did age correlate with survival?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median age, 28.0. is the same for both survivors and dead. However, the mean age is slightly higher for those that died. In light of the median, that means that the dead included more that were 'especially' old, compared to the survivor group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean and median age of passengers\n",
      "Mean and median age of passengers, by survival status(0 = died,1 = survived)\n",
      "                Age       \n",
      "               mean median\n",
      "Survived                  \n",
      "0         30.138532   28.0\n",
      "1         28.408392   28.0\n",
      "_____________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Mean and median age of passengers')\n",
    "\n",
    "pivot2=pd.pivot_table(df, index=['Survived'], values=['Age'], aggfunc={'mean','median'})\n",
    "print('Mean and median age of passengers, by survival status(0 = died,1 = survived)')\n",
    "print(pivot2)\n",
    "print('_____________________________________________________________________') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4)\n",
    "The purpose here is to find out whether there are multiple instances of a given last name.\n",
    "First,  identify each unique last name. For example, “Carr“ must not be confused with “Carrou”. Second, find a way to identify multiple instances of a given name. In order to do that, one must check, for each unique last name, whether there are any matches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge in this data is to identify the unique last names. In the .csv file, last names are not distinguished from first and middle names. Instead, each passengers’ full name is listed in a single string e.g. 'Mr. John Henry Rutherford'. The married women are registered with both their own first and middle names and their husband’s e.g. 'Mrs. Charles (Catherine Anne) Smith' \n",
    "\n",
    "To identify what is a last name, I read entries for 'Name' into a separate list of all the names. Then for each entry I split the name with 'rsplit'. For each line, I get the last name, split at the last whitespace into two segments they are stored into ‘last_name’. Each second segment is stored in ‘ls’. In order to avoid confusing names like “Carr” and “Carrou”, I add a space to the 'ls'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To distinguish between unique and multiple instances,  I first create an empty list “first_time_seen”. Ultimately first_time_seen is going to contain one and only one instance of each and every last name.  Second, I identify multiple instances by creating a second empty list “dup”, consisting of those unique names where there are confirmed multiple instances. As each last name is checked by the program it first checks to see if the name is a new unique name. \n",
    "\n",
    "If the name is not in ‘dup’ but is in ‘first_time_seen’  then we have just discovered a new duplicate. Therefore the program appends the name to ‘dup’. If the name being checked is neither in ‘dup‘ nor in first_time_seen, then it is an entirely new name (so far). It then is appended to ‘first_time_seen’.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:  In a subsequent discussion with Christian, he described simpler ways of checking for instances of the same last name: \n",
    "a. ‘reverse’, ‘string.split’   b. ‘new value counts’ \n",
    "However, my way of doing it was a good exercise for me in working out the underlying logic of for loops and elif. len(dup) would also give the count. The code and output below shows that there are indeed passengers who share a last name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passengers who shared a last name\n",
      "There are 133 duplicates: ['Palsson ', 'Planke ', 'Andersson ', 'Goodwin ', 'Fortune ', 'Moran ', 'Turpin ', 'Nasser ', 'White ', 'Nicola-Yarred ', 'Futrelle ', 'Ford ', 'Williams ', 'Panula ', 'Skoog ', 'Rice ', 'Johnson ', 'Sage ', 'Asplund ', 'Navratil ', 'Olsen ', 'Backstrom ', 'Harris ', 'Lefebre ', 'Zabour ', 'Attalah ', 'Strom ', 'Smith ', 'Baxter ', 'Allison ', 'Kantor ', 'Caldwell ', 'Goldsmith ', 'McCoy ', 'Graham ', 'Pears ', 'Brown ', 'Arnold-Franchi ', \"O'Brien \", 'Meyer ', 'Gustafsson ', 'Holverson ', 'Carter ', 'Newell ', 'Sandstrom ', 'Johansson ', 'Olsson ', 'Jussila ', 'Hakkarainen ', 'Hart ', 'Minahan ', 'Rosblom ', 'Richards ', 'Mellinger ', 'West ', 'Baclini ', 'Braund ', 'Bishop ', 'Hoyt ', 'Coutts ', 'Hagland ', 'Calic ', 'Castellana ', 'Webber ', 'Hippach ', 'Quick ', 'Elias ', 'Peter ', 'Cacic ', 'Beane ', 'Davies ', 'Taussig ', 'Flynn ', 'Kelly ', 'Silvey ', 'Thayer ', 'Bourke ', 'Impe ', 'Boulos ', 'Gordon ', 'Jacobsohn ', 'Laroche ', 'Murphy ', 'Danbom ', 'Lobb ', 'Becker ', 'Nakid ', 'Collyer ', 'Thorneycroft ', 'Harper ', 'Stanley ', 'Doling ', 'Hickman ', 'Frauenthal ', 'Coleff ', 'Chapman ', 'Barbara ', 'Hansen ', 'Morley ', 'Moubarek ', 'Taylor ', 'Larsson ', 'Jensen ', 'Oreskovic ', 'Renouf ', 'Allen ', 'Ryerson ', 'Crosby ', 'Abbott ', 'Herman ', 'Hamalainen ', 'Hocking ', 'Dick ', 'Ali ', 'Dean ', 'Keane ', 'Andrews ', 'Chambers ', 'Hays ', 'Moor ', 'Lam ', 'Mallet ', 'Yasbeck ', 'Saad ', 'Goldenberg ', 'Svensson ', 'Wick ', 'Daly ', 'Beckwith ', 'Carlsson ', 'Abelson ', 'Petroff ', 'Johnston ']\n",
      "_____________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Passengers who shared a last name')\n",
    "names=df['Name']\n",
    "dup=[]\n",
    "first_time_seen=[]\n",
    "x=0\n",
    "\n",
    "for each in names:\n",
    "    last_name=each.rsplit(maxsplit=1)\n",
    "    ls=last_name[1]\n",
    "    ls=ls+' ' # Adding a whitespace to 'ls' to ensure e.g. that Carr is not confused with Carrou\n",
    "# Check if it is in 'dup' already\n",
    "    if ls in dup:\n",
    "        #  do nothing\n",
    "        x=x #  x=x is a work-around because cannot have an empty line\n",
    "    elif (ls  in first_time_seen):\n",
    "        dup.append(ls) # Adding a comma between last names in list\n",
    "        x=x+1 # Each added last name counts as 1 more\n",
    "    else:\n",
    "        first_time_seen.append(ls) \n",
    "\n",
    "print('There are', x, 'duplicates:', dup) # list of shared last names\n",
    "print('_____________________________________________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) pivot-tabels   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many passengers travelled, respectively, first, second and third class\n",
      "        Name\n",
      "       count\n",
      "Pclass      \n",
      "1        216\n",
      "2        184\n",
      "3        487\n",
      "_____________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('How many passengers travelled, respectively, first, second and third class') \n",
    "\n",
    "pivot3=pd.pivot_table(df, index=['Pclass'], values=['Name'], aggfunc={'count'})\n",
    "print(pivot3)\n",
    "print('_____________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The passenger class with the most dead\n",
      "                 Name\n",
      "                count\n",
      "Survived Pclass      \n",
      "0        1         80\n",
      "         2         97\n",
      "         3        368\n",
      "1        1        136\n",
      "         2         87\n",
      "         3        119\n",
      "_____________________________________________________________________\n",
      "END\n"
     ]
    }
   ],
   "source": [
    "print('The passenger class with the most dead')\n",
    "\n",
    "pivot4=pd.pivot_table(df, index=['Survived', 'Pclass'], values=['Name'], aggfunc={'count'})\n",
    "print(pivot4)\n",
    "print('_____________________________________________________________________')\n",
    "print('END')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third class passengers had the most dead, 368.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCLUSION\n",
    "\n",
    "It is possible to download a .csv file and use Python and pandas to create dataframes, describe the data set and create descriptive statistics. This can be used to answer research questions about the Titanic's passengers, such as the correlation between age and survival, or whether different passenger classes had different survival rates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIBLIOGRAPHY\n",
    "Anderson, R. E. (1992) ‘ACM Code of Ethics and Professional Conduct’, Communications of the ACM, 35(5), pp. 94–99. doi: 10.1145/129875.129885.\n",
    "\n",
    "Association for Computing Machinery (2018) ‘Artifact Review and Badging’, p. 5. Available at: https://www.acm.org/publications/policies/artifact-review-badging.\n",
    "\n",
    "Downey, A. (2015) Think Python: How to Think Like a Computer Scientist. 2nd, version edn. Needham, MA: Green Tea Press. Available at: http://www.thinkpython2.com.\n",
    "\n",
    "Ignatow, G. and Milhacea (2018) Introduction to Text Mining. Menlo Park, CA: SAGE Publications, Inc.\n",
    "Lund, H. (2019) ‘OPD uge 36 SLIDES Modul_1_introduktion_Open Data Science’. Copenhagen: University of Copenhagen, Institute for information Studies, pp. 1–29.\n",
    "\n",
    "Thompson, S. A. and Warzel, C. (2019) ‘Twelve Million Phones, One Dataset, Zero Privacy’, The New York Times, 19 December. Available at: https://www.nytimes.com/interactive/2019/12/19/opinion/location-tracking-cell-phone.html.\n",
    "\n",
    "Vicente-Saez, R. and Martinez-Fuentes, C. (2018) ‘Open Science now: A systematic literature review for an integrated definition’, Journal of Business Research, 88(July), pp. 428–436. doi: 10.1016/j.jbusres.2017.12.043.\n",
    "\n",
    "Wilkinson, M. D. et al. (2016) ‘Comment: The FAIR Guiding Principles for scientific data management and stewardship’, Scientific Data, 3, pp. 1–9. doi: 10.1038/sdata.2016.18.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
